{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_GRID = '//div[@class=\"grid grid-cols-4 sm:grid-cols-5 md:grid-cols-6 lg:grid-cols-7 xl:grid-cols-8 h-100 p-3 text-slate-950\"]'\n",
    "TRAILBLAZER_ORDER = [\n",
    "    'Caelus - Destruction',\n",
    "    'Stelle - Destruction',\n",
    "    'Caelus - Preservation',\n",
    "    'Stelle - Preservation',\n",
    "    'Caelus - Harmony',\n",
    "    'Stelle - Harmony',\n",
    "]\n",
    "\n",
    "PATH_MAPPING = {\n",
    "    'mage': 'Erudition',\n",
    "    'rogue': 'The Hunt',\n",
    "    'warrior': 'Destruction',\n",
    "    'shaman': 'Harmony',\n",
    "    'warlock': 'Nihility',\n",
    "    'knight': 'Preservation',\n",
    "    'priest': 'Abundance'\n",
    "}\n",
    "\n",
    "class CharMiner:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.driver = None\n",
    "    \n",
    "    def create_driver(self):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.get(self.url)\n",
    "        self.driver.maximize_window()\n",
    "        self.wait = WebDriverWait(self.driver, 8)\n",
    "\n",
    "    def mine_char_data(self):\n",
    "        char_grid = self.wait.until(EC.presence_of_element_located((By.XPATH, CHAR_GRID)))\n",
    "        char_links = char_grid.find_elements(By.TAG_NAME, 'a')\n",
    "        trailblazer_idx = 0\n",
    "        return_dict = {}\n",
    "        for i in range(len(char_links)):\n",
    "            # char dict to store stuff\n",
    "            char_dict = {}\n",
    "\n",
    "            # refresh reference\n",
    "            char_grid = self.wait.until(EC.presence_of_element_located((By.XPATH, CHAR_GRID)))\n",
    "            char_links = char_grid.find_elements(By.TAG_NAME, 'a')\n",
    "            link = char_links[i]\n",
    "\n",
    "            # get the character name\n",
    "            name = link.find_element(By.TAG_NAME, 'div').get_attribute('innerText')\n",
    "\n",
    "            # handle trailblazer names\n",
    "            if 'Trailblazer' in name:\n",
    "                name = 'Trailblazer - ' + TRAILBLAZER_ORDER[trailblazer_idx]\n",
    "                trailblazer_idx += 1\n",
    "            \n",
    "            char_dict['Name'] = name\n",
    "            \n",
    "            # get image link\n",
    "            image_link = link.find_element(By.TAG_NAME, 'img').get_attribute('src')\n",
    "\n",
    "            char_dict['ImageLink'] = image_link\n",
    "\n",
    "            # download the image to /src/assets/character_icons\n",
    "            image_data = requests.get(image_link).content\n",
    "            image_path = f'../assets/character_icons/{name}.webp'\n",
    "            with open(image_path, 'wb') as f:\n",
    "                f.write(image_data)\n",
    "\n",
    "            char_dict['ImagePath'] = image_path\n",
    "\n",
    "            # get path\n",
    "            path = PATH_MAPPING[link.find_elements(By.TAG_NAME, 'img')[2].get_attribute('src').split('/')[-1].split('.')[0]]\n",
    "            char_dict['Path'] = path\n",
    "\n",
    "            # get the id matching that name\n",
    "            id = link.get_attribute('href').split('/')[-1]\n",
    "\n",
    "            return_dict[id] = char_dict\n",
    "        self.driver.close()\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m scraper \u001b[38;5;241m=\u001b[39m CharMiner(HAKUSHIN_URL)\n\u001b[0;32m      4\u001b[0m scraper\u001b[38;5;241m.\u001b[39mcreate_driver()\n\u001b[1;32m----> 5\u001b[0m char_dictionary \u001b[38;5;241m=\u001b[39m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmine_char_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 49\u001b[0m, in \u001b[0;36mCharMiner.mine_char_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# handle trailblazer names\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrailblazer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[1;32m---> 49\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrailblazer - \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mTRAILBLAZER_ORDER\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrailblazer_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     50\u001b[0m     trailblazer_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     52\u001b[0m char_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m name\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "HAKUSHIN_URL = 'https://hsr2.hakush.in/char'\n",
    "\n",
    "scraper = CharMiner(HAKUSHIN_URL)\n",
    "scraper.create_driver()\n",
    "char_dictionary = scraper.mine_char_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datamine/AvatarPromotionConfig.json') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "def organize_char_data(data, id):\n",
    "    ascensions = [\n",
    "        data[id]['0'],\n",
    "        data[id]['1'],\n",
    "        data[id]['2'],\n",
    "        data[id]['3'],\n",
    "        data[id]['4'],\n",
    "        data[id]['5'],\n",
    "        data[id]['6']\n",
    "    ]\n",
    "    \n",
    "    level_data = {}\n",
    "\n",
    "    # handle this in sections\n",
    "    for i in range(len(ascensions)):\n",
    "        current_ascension = ascensions[i]\n",
    "        min_level, max_level = 1, 0\n",
    "        if i == 0:\n",
    "            max_level = ascensions[i]['MaxLevel']\n",
    "        else:\n",
    "            min_level = ascensions[i-1]['MaxLevel']\n",
    "            max_level = ascensions[i]['MaxLevel']\n",
    "\n",
    "        base_atk = current_ascension['AttackBase']['Value']\n",
    "        base_def = current_ascension['DefenceBase']['Value']\n",
    "        base_hp = current_ascension['HPBase']['Value']\n",
    "\n",
    "        atk_add = current_ascension['AttackAdd']['Value']\n",
    "        def_add = current_ascension['DefenceAdd']['Value']\n",
    "        hp_add = current_ascension['HPAdd']['Value']\n",
    "        \n",
    "        # now that we know min and max level, fill in the values\n",
    "        for j in range(min_level, max_level+1):\n",
    "            stat_object = {}\n",
    "            current_atk = round((base_atk + atk_add*(j-1)), 3)\n",
    "            current_def = round((base_def + def_add*(j-1)), 3)\n",
    "            current_hp = round((base_hp + hp_add*(j-1)), 3)\n",
    "            level_tag = str(j)\n",
    "            if (j == min_level) & (j != 1):\n",
    "                # we're doing the x+\n",
    "                level_tag = str(min_level) + '+'\n",
    "            stat_object['ATK'] = current_atk\n",
    "            stat_object['DEF'] = current_def\n",
    "            stat_object['HP'] = current_hp\n",
    "            if j == 1:\n",
    "                stat_object['BaseSPD'] = current_ascension['SpeedBase']['Value']\n",
    "                stat_object['CritRate'] = current_ascension['CriticalChance']['Value']\n",
    "                stat_object['CritDamage'] = current_ascension['CriticalDamage']['Value']\n",
    "                stat_object['BaseAggro'] = current_ascension['BaseAggro']['Value']\n",
    "            level_data[level_tag] = stat_object\n",
    "    return level_data\n",
    "\n",
    "organize_char_data(json_data, '1303')\n",
    "\n",
    "for key in char_dictionary.keys():\n",
    "    char_dictionary[key]['BaseStats'] = organize_char_data(json_data, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/hsr_char_stats.json', 'w') as f:\n",
    "    json.dump(char_dictionary, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
